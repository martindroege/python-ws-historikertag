{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping von Hsozkult\n",
    "\n",
    "Das Ziel besteht darin, eine Suchanfrage an Hsozkult zu schicken und die erste Rezension, die wir erhalten in einer Text-Datei abzuspeichern.\n",
    "\n",
    "Als erstes binden wir die benötigten Bibliotheken ein: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`re` stellt Funktionen für sogenannte 'Reguläre Ausdrücke' zur Verfügung. `urllib` ist eine Standardbibliothek für den Umgang mit dem Internetprotokoll `http`. `bs4` steht für *BeautifulSoup* – die beste Bibliothek, um HTML zu parsen.\n",
    "\n",
    "Anschließend bauen wir uns eine URL zusammen: Sie besteht aus der eigentlichen Domain, aus dem Query-String, über den sich Suchanfragen ausführen lassen, und aus den eigentlichen Suchbegriffen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"http://www.hsozkult.de\"\n",
    "query_string = \"/searching/page?q=\"\n",
    "search_string = \"rezension grundwissenschaft\"\n",
    "search_string = search_string.replace(\" \", \"+\")\n",
    "url = base_url + query_string + search_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da keine Leerzeichen in einer URL vorhanden sein dürfen, müssen wir im Such-String alle Leerzeichen durch '+' ersetzen. Danach wird die URL aus den einzelnen Bestandteilen zusammengesetzt.\n",
    "\n",
    "Als nächstes senden wir mithilfe von `urlopen` eine Anfrage an Hsozkult. Die Antwort des Servers übergeben wir an *BeautifulSoup*, lassen es durch einen HTML-Parser verarbeiten und speichern das Ergebnis in der Variable `bs` ab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = urlopen(url)\n",
    "bs = BeautifulSoup(search, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daraufhin müssen wir einen Blick auf eine entsprechende Seite von Hsozkult werfen. Dabei stellen wir fest, dass die Tabelle mit den einzelnen Suchergebnissen sich in einem `div`-Kontainer der `class` \"hfn-list-itemtitle\" befindet. Aus der Ergebnis-Liste nehmen wir uns den ersten Treffern vor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/searching/id/rezbuecher-28479?title=t-kahlert-unternehmungen-grossen-stils&q=rezension grundwissenschaft&sort=&fq=&total=19&recno=1&subType=reb'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = bs.find_all('div', {'class': 'hfn-list-itemtitle'})\n",
    "\n",
    "first_hit = results[0].find('a')['href']\n",
    "first_hit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir suchen in diesem ersten Treffer nach einem Link `<a>` mit dem Attribut `href`, lesen das aus und speichern es in der Variable `first_hit` ab. Damit wissen wir, hinter welchem Link sich die erste gefundene Rezension verbirgt.\n",
    "\n",
    "Wir bauen daher anschließend erneut eine URL zusammen, diesmal aus der `base_url` und unserem `first_hit`. Da sich darin aber ein Leerzeichen befindet, müssen wir es wieder durch ein den entsprechenden Escape-Code '%20' ersetzen.\n",
    "\n",
    "Haben wir das erledigt, schicken wir erneute eine Anfrage an Hsozkult, diesmal für die Rezension, und parsen das Ganze mit *BeautifulSoup*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_url = base_url + first_hit\n",
    "link_url = link_url.replace(\" \", \"%20\")\n",
    "\n",
    "review_html = urlopen(link_url)\n",
    "bs_review = BeautifulSoup(review_html, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unser Ziel ist es, den eigentlichen Text der Rezension zusammen mit den dazugehörigen bibliographischen Metadaten in einer Datei zu speichern. Um mit *BeautifulSoup* auf die einschlägigen HTML-Tags zugreifen zu können, werfen wir einen Blick auf den Quelltext der Seite. Auf diese Weise stellen wir fest, dass sich alle bibliographischen Metadaten in einer Tabelle je in einem `div` mit dem Klassen-Attribut `class=\"hfn-item-metarow\"` befinden. Eine Spalte gibt Auskunft darüber, um welchen Datentyp es sich handelt, also bspw. 'Autor(en)', 'Titel', 'Erschienen' etc. Die andere Spalte enthält die entsprechenden Angaben. Es bietet sich daher an, anhand dieser Angaben ein *Dictionary* zu erstellen.\n",
    "\n",
    "Dafür lesen wir jede Zeile der Tabelle aus und übergeben die Werte einem *Dictionary* namens `meta_data`. Wir interessieren uns dabei nur für die ausgegebenen Texte innerhalb der *Tags*. Und wir entfernen den unnötigen *White space*. Wenn eine Spalte keine Angaben enthält, übernehmen wir sie auch nicht ins *Dictionary*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Autor(en)': 'Kahlert, Torsten',\n",
       " 'Erschienen': 'Berlin  2017: be.bra Verlag',\n",
       " 'ISBN': '978-3-95410-089-7',\n",
       " 'Preis': '€ 40,00',\n",
       " 'Titel': '„Unternehmungen großen Stils“.  Wissenschaftsorganisation, Objektivität und Historismus im 19. Jahrhundert',\n",
       " 'Umfang': '384 S.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_data = {}\n",
    "\n",
    "for key, val in bs_review.find_all('div', {'class': 'hfn-item-metarow'}):\n",
    "    k = key.get_text()\n",
    "    k = k.strip()\n",
    "    v = val.get_text()\n",
    "    v = v.strip()\n",
    "    v = re.sub(r\"[\\n\\t]+\", \" \", v)\n",
    "    if k != '':\n",
    "        meta_data[k] = v\n",
    "\n",
    "meta_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun kümmern wir uns um den Rezensenten. Die Angaben dazu verstecken sich im `div` mit der Klasse `hfn-item-creator`. Auch hier interessieren wir uns nur für den Text. Das Institut etc. spielt für uns aber keine Rolle. Wir zerlegen also den String `review_author` an den Kommas und greifen nur auf den ersten Abschnitt der geschaffenen Liste zurück."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jan Ruhkopf'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_author = bs_review.find('div', {'class': 'hfn-item-creator'})\n",
    "review_author = review_author.get_text()\n",
    "review_author = review_author.strip().split(',')[0]\n",
    "\n",
    "review_author"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt lesen wir den eigentlichen Text der Besprechung aus. In dem `div` mit der Klasse `hfn-item-fulltext` befinden sich gleich mehrere Paragraphen. Diese suchen wir mit zwei `find`- bzw. `find_all`-Methoden des *BeautifulSoup*-Objekts. Das Ergebnis ist eine Liste mit den einzelnen Paragraphen, die wir in der Variable `review` abspeichern. Danach nutzen wir eine besondere Python-Konstruktion, die *list comprehension*. Mit ihrer Hilfe erstellen wir eine neue Liste `review_content`, in der der Text der einzelnen Paragraphen-*tags* hinterlegt wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = bs_review.find('div', {'class': 'hfn-item-fulltext'}).find_all('p')\n",
    "\n",
    "review_content = [paragraph.get_text() for paragraph in review]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zu guter letzt speichern wir das alles in der Datei `rezension.txt` ab. Wir öffen es mit der Funktion `with open(...) as x:`, die sicherstellt, dass das *File*-Objekt am Ende auch wieder geschlossen wird.\n",
    "\n",
    "In dem Block unterscheiden wir, ob es sich um einen Sammelband handelt oder um eine Monographie. Dementsprechend formatieren wir den String, den wir durch Interpolation aus den Metadaten und `review_author` zusammensetzen. Danach kommen zwei *newlines* und der Rezensionstext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rezension.txt', 'w', encoding='utf-8') as f:\n",
    "    if 'Hrsg. v.' in meta_data.keys():\n",
    "        f.write(\"{}: Rezension von: {} (Hg.): {}, {}.\".format(\n",
    "            review_author,\n",
    "            meta_data['Hrsg. v.'],\n",
    "            meta_data['Titel'],\n",
    "            meta_data['Erschienen']))\n",
    "    else:\n",
    "        f.write(\"{}: Rezension von: {}: {}, {}.\".format(\n",
    "            review_author,\n",
    "            meta_data['Autor(en)'],\n",
    "            meta_data['Titel'],\n",
    "            meta_data['Erschienen']))\n",
    "    f.write(\"\\n\\n\")\n",
    "    f.write(\"\\n\".join(review_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fertig ist unser Hsozkult-Scraper!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
